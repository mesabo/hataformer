{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_datasets(dataset_map, base_dir=\"../data/original\", save_dir=\"../data/raw\"):\n",
    "    \"\"\"\n",
    "    Preprocess datasets by renaming the target column, resampling datetime data to hourly,\n",
    "    and saving the processed datasets.\n",
    "    \n",
    "    Args:\n",
    "        dataset_map (dict): A dictionary where keys are dataset names and values are target column names.\n",
    "        base_dir (str): Base directory where the original datasets are stored.\n",
    "        save_dir (str): Directory where the processed datasets will be saved.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Ensure save directory exists\n",
    "\n",
    "    for dataset_name, target_column in dataset_map.items():\n",
    "        file_path = os.path.join(base_dir, dataset_name)\n",
    "        \n",
    "        # Check file extension (supports CSV and TXT)\n",
    "        if dataset_name.endswith(\".csv\"):\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif dataset_name.endswith(\".txt\"):\n",
    "            # Use semicolon as the delimiter for txt files\n",
    "            df = pd.read_csv(file_path, delimiter=\";\")\n",
    "        else:\n",
    "            print(f\"Unsupported file format for {dataset_name}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Rename the target column to \"conso\"\n",
    "        if target_column in df.columns:\n",
    "            df.rename(columns={target_column: \"conso\"}, inplace=True)\n",
    "        else:\n",
    "            print(f\"Target column '{target_column}' not found in {dataset_name}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Handle datetime column\n",
    "        if dataset_name.endswith(\".txt\") and 'Date' in df.columns and 'Time' in df.columns:  # For txt files\n",
    "            # Combine 'Date' and 'Time' into a single 'date' column\n",
    "            df['date'] = pd.to_datetime(df['Date'] + ' ' + df['Time'], format=\"%d/%m/%Y %H:%M:%S\", errors='coerce')\n",
    "            # Drop the original 'Date' and 'Time' columns\n",
    "            df.drop(columns=['Date', 'Time'], inplace=True)\n",
    "        elif 'date' in df.columns:  # For other files with a single 'date' column\n",
    "            df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "        else:\n",
    "            print(f\"No valid datetime columns found in {dataset_name}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Drop rows with invalid dates\n",
    "        df.dropna(subset=['date'], inplace=True)\n",
    "        \n",
    "        # Resample to hourly\n",
    "        df.set_index('date', inplace=True)\n",
    "        df = df.resample('H').asfreq().reset_index()  # Resample to hourly without aggregating\n",
    "        \n",
    "        # Save the processed dataset\n",
    "        # Ensure the filename always has a .csv extension\n",
    "        output_name = os.path.splitext(dataset_name)[0] + \".csv\"\n",
    "        save_path = os.path.join(save_dir, output_name)\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\"Processed dataset saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_map = {\n",
    "    #\"electricity.csv\": \"OT\",\n",
    "    #\"energy.csv\": \"Appliances\",\n",
    "    #\"household.txt\": \"Global_active_power\",\n",
    "    #\"tetuancity.csv\": \"Power Consumption\",\n",
    "    \"ETTh1.csv\": \"OT\",\n",
    "    \"ETTh2.csv\": \"OT\",\n",
    "    \"ETTm1.csv\": \"OT\",\n",
    "    \"ETTm2.csv\": \"OT\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b8/20d44f8s2m19rh4cjlzb01lc0000gn/T/ipykernel_90233/3736946115.py:50: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df = df.resample('H').asfreq().reset_index()  # Resample to hourly without aggregating\n",
      "/var/folders/b8/20d44f8s2m19rh4cjlzb01lc0000gn/T/ipykernel_90233/3736946115.py:50: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df = df.resample('H').asfreq().reset_index()  # Resample to hourly without aggregating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dataset saved to ../data/raw/ETTh1.csv\n",
      "Processed dataset saved to ../data/raw/ETTh2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b8/20d44f8s2m19rh4cjlzb01lc0000gn/T/ipykernel_90233/3736946115.py:50: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df = df.resample('H').asfreq().reset_index()  # Resample to hourly without aggregating\n",
      "/var/folders/b8/20d44f8s2m19rh4cjlzb01lc0000gn/T/ipykernel_90233/3736946115.py:50: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df = df.resample('H').asfreq().reset_index()  # Resample to hourly without aggregating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dataset saved to ../data/raw/ETTm1.csv\n",
      "Processed dataset saved to ../data/raw/ETTm2.csv\n"
     ]
    }
   ],
   "source": [
    "preprocess_datasets(dataset_map)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
