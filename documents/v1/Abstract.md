# üìù Abstract

Long-term forecasting of multivariate time series is a critical task in Internet of Things (IoT) applications, where sensor data often exhibits dynamic, multi-scale temporal patterns and irregular periodic structures. While Transformer-based models have shown strong potential, their effectiveness is constrained by two longstanding challenges: (1) the lack of temporal semantic awareness in static positional encodings, and (2) the rigidity of attention mechanisms that fail to flexibly adapt across local and global temporal scopes. In this work, we propose **TAPFormer**, a decoder-only Transformer architecture designed for accurate, efficient, and interpretable long-term forecasting. TAPFormer introduces a novel **Temporal-Proj** module that replaces fixed positional encodings with a learnable projection of raw time features, enabling the model to directly incorporate contextual semantics such as hour, weekday, and seasonality. Additionally, TAPFormer employs a **Dual-Scale Attention mechanism** that splits attention heads across local and global temporal contexts, dynamically fused through **gating** and enhanced via a **soft locality bias**. This design allows for flexible temporal dependency modeling without increasing computational complexity. Forecasting is performed in a single forward pass, avoiding the inefficiencies of recursive or autoregressive pipelines. Extensive experiments on eight real-world benchmarks demonstrate that TAPFormer achieves **state-of-the-art performance**, improving forecasting accuracy by an average of **+5.7% in MSE reduction** over leading baselines, while reducing inference cost by up to **40%**. Our results confirm TAPFormer as a compact yet expressive solution for scalable time series forecasting in real-world IoT scenarios.

---

**Keywords**: Time Series Forecasting, Internet of Things (IoT), Transformer Models, Positional Encoding, Dual-Scale
Attention, Temporal Representation Learning, Long-Term Prediction, Multivariate Time Series.
